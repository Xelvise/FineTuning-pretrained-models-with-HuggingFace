{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate peft bitsandbytes -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T14:54:39.276795Z","iopub.execute_input":"2024-05-15T14:54:39.277232Z","iopub.status.idle":"2024-05-15T14:54:57.779409Z","shell.execute_reply.started":"2024-05-15T14:54:39.277199Z","shell.execute_reply":"2024-05-15T14:54:57.778236Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\nfrom huggingface_hub import login\n\nmodel_name = 'mistralai/Mistral-7B-Instruct-v0.2'\nlogin(token='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')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:30.255355Z","iopub.execute_input":"2024-05-15T14:55:30.255763Z","iopub.status.idle":"2024-05-15T14:55:37.461586Z","shell.execute_reply.started":"2024-05-15T14:55:30.255729Z","shell.execute_reply":"2024-05-15T14:55:37.460777Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_quantized_model(model_name: str):\n    \"\"\"\n    :param model_name: Name or path of the model to be loaded.\n    :return: Loaded quantized model.\n    \"\"\"\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,    # this enables double (or nested) quantization which applies a 2nd quantization after the inital one. It saves an additional 0.4bits per parameter\n        bnb_4bit_quant_type=\"nf4\",    # this specifies the type of 4-bit quantization to be used. In this case, `nf4` refers to normalized float4 which is the default quantization type.\n        bnb_4bit_compute_dtype=torch.bfloat16    # this determines the compute datatype used during computation. It specifies the use of the 'bfloat16' dtype for faster training. The compute_dtype can be chosen from options like float16, bfloat16, float32, bfloat32 etc. And this configuration is needed because while 4-bit BitsAndBytes stores weights in 4-bits, the computation still happens in 16 or 32 bits. The matrix multiplication and training will be faster, if one uses 16-bit compute datatype\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        quantization_config=bnb_config)\n#     ).to(device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:37.463558Z","iopub.execute_input":"2024-05-15T14:55:37.463972Z","iopub.status.idle":"2024-05-15T14:55:37.470871Z","shell.execute_reply.started":"2024-05-15T14:55:37.463946Z","shell.execute_reply":"2024-05-15T14:55:37.469701Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### `load_in_4bit = True` loads Mistral-7B in 4-bits precision. This means that the weights and activations of the model are represented using 4-bits instead of the usual 32-bits. This can significantly reduce the memory footprint of the model.\n\n#### 4-bit precision models can use up to 16-times less memory, and can be up to twice faster than full precision models. But if you need the highest possible accuracy, then you may want to use the full precision models. However in almost all consumer GPUs including that of Google colab, for running 13B or even 7B params model, it's better to apply quantization\n\n#### `bnb_4bit_use_double_quant = True` enables double (or nested) quantization which applies a second quantization after the inital one. It saves an additional 0.4bits per parameter\n\n#### `bnb_4bit_quant_type=\"nf4\"` specifies the type of 4-bit quantization to be used. In this case, `nf4` refers to normalized float4 which is the default quantization type.\n\n#### This quantization type, however, is only compatible with GPUs. In other words, it's not possible to quantize models in 4-bit on a CPU. Pretty much any GPU could be used to run the 4-bit quantization (so long as you have cuda installed).\n\n#### `bnb_4bit_compute_dtype = torch.bfloat16` determines the compute datatype used during computation. It specifies the use of the `bfloat16` dtype for faster training. The compute_dtype can be chosen from options like float16, bfloat16, float32, bfloat32 etc. This configuration is needed because while 4-bit BitsAndBytes stores weights in 4-bits, the computation is performed in 16 or 32 bits. The matrix multiplication and training becomes faster, when 16-bit compute datatype is used\n\n#### And just to reiterate, the computation isn't carried out in 4-bit. The weights and activations are only compressed to that format, while the computation is performed in the native datatype of the model","metadata":{}},{"cell_type":"code","source":"def initialize_tokenizer(model_name: str):\n    \"\"\"\n    Initialize the tokenizer with the specified model_name.\n\n    :param model_name: Name or path of the model for tokenizer initialization.\n    :return: Initialized tokenizer.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:37.472321Z","iopub.execute_input":"2024-05-15T14:55:37.472766Z","iopub.status.idle":"2024-05-15T14:55:37.481376Z","shell.execute_reply.started":"2024-05-15T14:55:37.472711Z","shell.execute_reply":"2024-05-15T14:55:37.480569Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nmodel = load_quantized_model(model_name)\ntokenizer = initialize_tokenizer(model_name)\n\n# Define stop token ids\nstop_token_ids = [0]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:55:37.483052Z","iopub.execute_input":"2024-05-15T14:55:37.483378Z","iopub.status.idle":"2024-05-15T14:57:59.914436Z","shell.execute_reply.started":"2024-05-15T14:55:37.483319Z","shell.execute_reply":"2024-05-15T14:57:59.913623Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbf6fb679b444c988e0486756748a70a"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77dcf74d8b914ff4ab5013bb9c460440"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cfc04b8c12447298d0406a5a4fc49d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0add81ea51024582ab71d276353a48cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8440c29fc40d4347b3d98a9a0362c181"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb5566d6eae64f359868d159f15c3ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da72818fba894deab98d06e6e13fc79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82ca98254664bec854a436a4cb84518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c3c9ee38b14f71a012dfe861ac3680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c85b6431e39437d847bcbea93a5364d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"049921281bd04c84b3bc875607b0215c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca651c562b6248a58a46a0c10cc9be5b"}},"metadata":{}}]},{"cell_type":"code","source":"def inference(text:str, max_tokens:int=200):\n    text = '[INST] ' + text.strip() + ' [/INST]' \n    model_input = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n    generated_ids = model.generate(**model_input, max_new_tokens=max_tokens, do_sample=True)\n    decoded = tokenizer.batch_decode(generated_ids)\n    return decoded[0]\n\nimport warnings; warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T14:57:59.915956Z","iopub.execute_input":"2024-05-15T14:57:59.916509Z","iopub.status.idle":"2024-05-15T14:57:59.922684Z","shell.execute_reply.started":"2024-05-15T14:57:59.916475Z","shell.execute_reply":"2024-05-15T14:57:59.921745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(inference(\"Tell me all you know about Parameter-Efficient Fine-tuning?\", 500))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T15:02:01.796652Z","iopub.execute_input":"2024-05-15T15:02:01.797044Z","iopub.status.idle":"2024-05-15T15:02:44.676128Z","shell.execute_reply.started":"2024-05-15T15:02:01.797015Z","shell.execute_reply":"2024-05-15T15:02:44.675176Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n2024-05-15 15:02:05.833199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-15 15:02:05.833298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-15 15:02:06.006139: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[INST] Tell me all you know about Parameter-Efficient Fine-tuning? [/INST] Parameter-Efficient Fine-tuning (PEFT) is a method used for adapting pre-trained models to new tasks with minimal additional computational cost. The main idea behind PEFT is to keep the weights of the pre-trained model frozen and only update a small subset of model parameters, known as adaptation token or projection heads.\n\nIn PEFT, the model is fine-tuned by adding a few new layers on top of the pre-trained model. These new layers can be called mismatch embeddings, promoter, or adapters. They are typically lightweight and can be trained using a small dataset. By updating only these new layers, the model can learn task-specific features while preserving the general knowledge learned from the pre-training stage.\n\n PEFT has several advantages, some of them are:\n\n1. Reduced computational cost as fewer parameters need to be updated compared to full fine-tuning.\n2. Improved generalization ability as the model maintains the original pre-trained parameters.\n3. Faster training and higher convergence as the small number of updated parameters requires less data for effective optimization.\n4. Flexibility in being applied to various tasks such as text generation, machine translation, and classification without the need for extensive architecture modifications.\n\nSome popular PEFT methods include Prompt Tuning, LLAMA, LoRa, and DINO. These methods vary in their design and implementation but generally follow the PEFT philosophy of updating a small set of new parameters. PEFT is an active area of research in the machine learning community and continues to gain popularity due to its efficiency and flexibility.</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}